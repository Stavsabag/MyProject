{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b548b21a-00c7-4f62-8f82-718c273a8887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script processes feature changes in a Twitter user dataset and calculates statistical measures for the changes.\n",
    "# It connects to a SQLite database, retrieves the feature changes for each user, and computes various statistics.\n",
    "# The computed statistics are then stored in a separate table in the database.\n",
    "# The script operates in chunks to handle large datasets efficiently.\n",
    "\n",
    "import sqlite3\n",
    "import statistics\n",
    "import datetime\n",
    "from scipy.stats import skew\n",
    "import numpy as np\n",
    "\n",
    "# define function to process feature changes\n",
    "def process_feature_changes(rows, feature_name):\n",
    "    user_changes = {}\n",
    "    for row in rows:\n",
    "        user_id = row['user_id']\n",
    "        prev_value, curr_value = int(row['previous']), int(row['current'])\n",
    "        diff = curr_value - prev_value\n",
    "        user_changes.setdefault(user_id, []).append(diff)\n",
    "        num_changes = len(user_changes[user_id]) if user_changes[user_id] else 0\n",
    "\n",
    "    return {user_id: {\n",
    "                \"max_diff\": max(user_changes[user_id], default=0),\n",
    "                \"min_diff\": min(user_changes[user_id], default=0),\n",
    "                \"median_diff\": statistics.median(user_changes[user_id]) if user_changes[user_id] else 0,\n",
    "                \"average_diff\": statistics.mean(user_changes[user_id]) if user_changes[user_id] else 0,\n",
    "                \"std_diff\": statistics.stdev(user_changes[user_id]) if num_changes > 1 else 0 if num_changes == 1 else -1,\n",
    "                \"var_diff\": statistics.variance(user_changes[user_id]) if num_changes > 1 else 0 if num_changes == 1 else -1,\n",
    "                \"skew_diff\": skew(user_changes[user_id], bias=False) if num_changes > 1 and statistics.stdev(user_changes[user_id]) != 0 else 100,\n",
    "                \"range_diff\": max(user_changes[user_id], default=0) - min(user_changes[user_id], default=0),\n",
    "                \"mad_diff\": statistics.median(\n",
    "                    [abs(x - statistics.median(user_changes[user_id])) for x in user_changes[user_id]]) if user_changes[\n",
    "                    user_id] else 0 if num_changes == 1 else -1,\n",
    "                \"cv_diff\": statistics.stdev(user_changes[user_id]) / statistics.mean(user_changes[user_id]) if num_changes > 1 and statistics.mean(user_changes[user_id]) != 0 else 0 if len(user_changes[user_id]) == 1 else -1,\n",
    "            }\n",
    "            for user_id in user_changes}\n",
    "\n",
    "\n",
    "# connect to database\n",
    "conn = sqlite3.connect('TwitterUserChanges.db')\n",
    "conn.row_factory = sqlite3.Row\n",
    "\n",
    "# create table if not exists\n",
    "conn.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS processed_data (\n",
    "        user_id TEXT,\n",
    "        feature_name TEXT,\n",
    "        value REAL,\n",
    "        test_time TEXT,\n",
    "        UNIQUE(user_id, feature_name)\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# get current time\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# define features and new_features\n",
    "features = [\"followers_count\", \"friends_count\", \"listed_count\" ,\"statuses_count\",\"favorites_count\"]\n",
    "new_features = [\n",
    "\"max_diff\",\n",
    "\"min_diff\",\n",
    "\"median_diff\",\n",
    "\"average_diff\",\n",
    "\"std_diff\",\n",
    "\"var_diff\",\n",
    "\"skew_diff\",\n",
    "\"range_diff\",\n",
    "\"mad_diff\",\n",
    "\"cv_diff\",\n",
    "]\n",
    "\n",
    "# get distinct user IDs\n",
    "cursor = conn.execute(\"SELECT DISTINCT user_id FROM users\")\n",
    "user_ids = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "# Define the chunk size for batch insertion\n",
    "chunk_size = len(user_ids)\n",
    "chunk_amount = len(user_ids)/chunk_size\n",
    "\n",
    "\n",
    "# Loop through user IDs in chunks\n",
    "for i in range(0, len(user_ids), chunk_size):\n",
    "    user_ids_chunk = user_ids[i:i+chunk_size]\n",
    "    remaining_iterations2 = chunk_size\n",
    "\n",
    "    # Get all the rows for the user_ids_chunk for all features at once\n",
    "    rows = conn.execute(\"SELECT user_id, previous, current, feature FROM changes WHERE user_id IN ({}) AND feature IN ({})\".format(','.join('?' for _ in user_ids_chunk), ','.join('?' for _ in features)), tuple(user_ids_chunk + features)).fetchall()\n",
    "\n",
    "    # Group the rows by user_id and feature\n",
    "    grouped_rows = {}\n",
    "    for row in rows:\n",
    "        grouped_rows.setdefault(row['user_id'], {}).setdefault(row['feature'], []).append(row)\n",
    "\n",
    "    # Create an array to store processed data for the chunk of users\n",
    "    processed_data_chunk = np.zeros((chunk_size*len(features)*len(new_features), 3), dtype=np.object)\n",
    "    j = 0\n",
    "\n",
    "    # Loop through each user in the chunk\n",
    "    for user_id in user_ids_chunk:\n",
    "        # Loop through each feature for the user\n",
    "        for k, feature in enumerate(features):\n",
    "            # Get the rows for the user_id and feature\n",
    "            rows = grouped_rows.get(user_id, {}).get(feature, [])\n",
    "\n",
    "            # Process the feature changes and get the values for new features\n",
    "            processed_feature_changes = process_feature_changes(rows, feature)\n",
    "\n",
    "            # Insert the values into the processed data array\n",
    "            for l, stat_name in enumerate(new_features):\n",
    "                value = processed_feature_changes.get(user_id, {}).get(stat_name, 0 if stat_name in {\"max_diff\", \"min_diff\", \"median_diff\", \"average_diff\", \"mode_diff\", \"range_diff\", \"mad_diff\"} else -1 if stat_name in {\"std_diff\", \"var_diff\", \"cv_diff\"} else 100 if stat_name == \"skew_diff\" else None)\n",
    "                processed_data_chunk[j, 0] = user_id\n",
    "                processed_data_chunk[j, 1] = \"Activity \" + feature + \" \" + stat_name\n",
    "                processed_data_chunk[j, 2] = value\n",
    "                j += 1\n",
    "                if j == len(new_features) :\n",
    "                    break\n",
    "\n",
    "        remaining_iterations2-=1\n",
    "        print((\"Iterations left:\", remaining_iterations2))\n",
    "        if j == chunk_size*len(features)*len(new_features):\n",
    "            break\n",
    "\n",
    "    # Insert the chunk of data into the database\n",
    "    cursor = conn.cursor()\n",
    "    values = [(row[0], row[1], row[2], current_time) for row in processed_data_chunk if any(row)]\n",
    "    cursor.executemany(\n",
    "        \"INSERT OR REPLACE INTO processed_data (user_id, feature_name, value, test_time) VALUES (?, ?, ?, ?)\", values)\n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "    chunk_amount -= 1\n",
    "    print(\"chunk_left:\", chunk_amount)\n",
    "\n",
    "# Close the connection to the database\n",
    "conn.close()\n",
    "\n",
    "### Extraction of additional features ###\n",
    "\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect('TwitterUserChanges.db')\n",
    "conn.row_factory = sqlite3.Row\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Fetch the data from the current_values table\n",
    "cursor.execute(\"SELECT user_id, followers_count, friends_count, listed_count, statuses_count, favorites_count, user_verification_status, protected, user_created_at FROM current_values\")\n",
    "data = cursor.fetchall()\n",
    "\n",
    "# Get the current datetime\n",
    "current_datetime = datetime.datetime.now(datetime.timezone.utc)\n",
    "\n",
    "# Process the data and calculate new features\n",
    "processed_data = []\n",
    "for row in data:\n",
    "    user_id = row['user_id']\n",
    "    followers_count = row['followers_count']\n",
    "    following_count = row['friends_count']\n",
    "    listed_count = row['listed_count']\n",
    "    statuses_count = row['statuses_count']\n",
    "    favorites_count = row['favorites_count']\n",
    "    user_verification_status = row['user_verification_status']\n",
    "    protected = row['protected']\n",
    "    user_created_at = datetime.datetime.strptime(row['user_created_at'], \"%Y-%m-%d %H:%M:%S%z\")\n",
    "\n",
    "    days_since_creation = (current_datetime - user_created_at).days\n",
    "\n",
    "    ratio_followers_following = followers_count / (following_count + 1)\n",
    "    has_followers = 1 if followers_count > 0 else 0\n",
    "    has_following = 1 if following_count > 0 else 0\n",
    "    tweets_per_day = statuses_count / days_since_creation\n",
    "    has_statuses = 1 if statuses_count > 0 else 0\n",
    "    liked_other_content = 1 if favorites_count > 0 else 0\n",
    "    verified = 0 if user_verification_status == 'Not verified' else 1\n",
    "    private = 0 if protected == 'Not protected' else 1\n",
    "\n",
    "    processed_data.append((user_id, 'Followers Count', followers_count))\n",
    "    processed_data.append((user_id, 'Following Count', following_count))\n",
    "    processed_data.append((user_id, 'Ratio Followers Following', ratio_followers_following))\n",
    "    processed_data.append((user_id, 'Has Followers', has_followers))\n",
    "    processed_data.append((user_id, 'Has Following', has_following))\n",
    "    processed_data.append((user_id, 'Listed Count', listed_count))\n",
    "    processed_data.append((user_id, 'Statuses Count', statuses_count))\n",
    "    processed_data.append((user_id, 'Favorites Count', favorites_count))\n",
    "    processed_data.append((user_id, 'Tweets per Day', tweets_per_day))\n",
    "    processed_data.append((user_id, 'Has Statuses', has_statuses))\n",
    "    processed_data.append((user_id, 'Liked Other Content', liked_other_content))\n",
    "    processed_data.append((user_id, 'Verified', verified))\n",
    "    processed_data.append((user_id, 'Private', private))\n",
    "\n",
    "# Insert the processed data into the processed_data table\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "cursor.executemany(\"INSERT OR REPLACE INTO processed_data (user_id, feature_name, value, test_time) VALUES (?, ?, ?, ?)\", [(row[0], row[1], row[2], current_time) for row in processed_data])\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
