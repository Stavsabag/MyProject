{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcca3dc-e928-421e-9fad-a6d0f31d4e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script processes feature changes in a Twitter user dataset and calculates various statistical measures based on Levenshtein distance between previous and current values. It stores the processed data in a SQLite database.\n",
    "\n",
    "The process_feature_changes function calculates the Levenshtein distance for each feature change and computes statistical measures such as maximum, minimum, median, average, standard deviation, variance, mode, edit distance per character, skewness, range, median absolute deviation, and coefficient of variation.\n",
    "\n",
    "The script connects to the database, defines the necessary tables, retrieves the user IDs, and processes the feature changes in chunks. It then inserts the processed data into the 'processed_data' table.\n",
    "\n",
    "Please make sure to update the database connection details and adjust the features and new_features lists according to your dataset.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import sqlite3\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "import statistics\n",
    "import datetime\n",
    "from scipy.stats import skew\n",
    "import numpy as np\n",
    "\n",
    "# define function to process feature changes\n",
    "def process_feature_changes(rows, feature_name):\n",
    "    user_changes = {}\n",
    "    for row in rows:\n",
    "        user_id = row['user_id']\n",
    "        prev_value, curr_value = row['previous'].strip(), row['current'].strip()\n",
    "        user_changes.setdefault(user_id, []).append(levenshtein_distance(prev_value, curr_value))\n",
    "        num_changes = len(user_changes[user_id]) if user_changes[user_id] else 0\n",
    "\n",
    "    return {user_id: {\n",
    "                \"num_changes\": num_changes,\n",
    "                \"max_edit_distance\": max(user_changes[user_id], default=-1),\n",
    "                \"min_edit_distance\": min(user_changes[user_id], default=-1),\n",
    "                \"median_edit_distance\": statistics.median(user_changes[user_id]) if user_changes[user_id] else -1,\n",
    "                \"average_edit_distance\": statistics.mean(user_changes[user_id]) if user_changes[user_id] else -1,\n",
    "                \"std_edit_distance\": statistics.stdev(user_changes[user_id]) if num_changes > 1 else 0 if num_changes == 1 else -1,\n",
    "                \"var_edit_distance\": statistics.variance(user_changes[user_id]) if num_changes > 1 else 0 if num_changes == 1 else -1,\n",
    "                \"mode_edit_distance\": statistics.mode(user_changes[user_id]) if user_changes[user_id] else -1,\n",
    "                \"edit_distance_per_character\": statistics.mean(user_changes[user_id]) / (\n",
    "                    (len(prev_value) + len(curr_value)) / 2) if user_changes[user_id] else -1,\n",
    "                \"skew_edit_distance\": skew(user_changes[user_id], bias=False) if num_changes > 1 and statistics.stdev(user_changes[user_id]) != 0 else 100,\n",
    "                \"range_edit_distance\": max(user_changes[user_id], default=-1) - min(user_changes[user_id], default=-1),\n",
    "                \"mad_edit_distance\": statistics.median(\n",
    "                    [abs(x - statistics.median(user_changes[user_id])) for x in user_changes[user_id]]) if user_changes[\n",
    "                    user_id] else 0 if num_changes == 1 else -1,\n",
    "                \"cv_edit_distance\": statistics.stdev(user_changes[user_id]) / statistics.mean(user_changes[user_id]) if num_changes > 1 else 0 if len(user_changes[user_id]) == 1 else -1,\n",
    "            }\n",
    "            for user_id in user_changes}\n",
    "\n",
    "# connect to database\n",
    "conn = sqlite3.connect('TwitterUserChanges.db')\n",
    "conn.row_factory = sqlite3.Row\n",
    "\n",
    "# create table if not exists\n",
    "conn.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS processed_data (\n",
    "        user_id TEXT,\n",
    "        feature_name TEXT,\n",
    "        value REAL,\n",
    "        test_time TEXT,\n",
    "        UNIQUE(user_id, feature_name)\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# get current time\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# define features and new_features\n",
    "features = [\"full_name\", \"username\", \"description\"]\n",
    "new_features = [\n",
    "\"num_changes\",\n",
    "\"max_edit_distance\",\n",
    "\"min_edit_distance\",\n",
    "\"median_edit_distance\",\n",
    "\"average_edit_distance\",\n",
    "\"std_edit_distance\",\n",
    "\"var_edit_distance\",\n",
    "\"mode_edit_distance\",\n",
    "\"edit_distance_per_character\",\n",
    "\"skew_edit_distance\",\n",
    "\"range_edit_distance\",\n",
    "\"mad_edit_distance\",\n",
    "\"cv_edit_distance\",\n",
    "]\n",
    "\n",
    "# get distinct user IDs\n",
    "cursor = conn.execute(\"SELECT DISTINCT user_id FROM users\")\n",
    "user_ids = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "\n",
    "# Define the chunk size for batch insertion\n",
    "chunk_size = len(user_ids)\n",
    "chunk_amount = len(user_ids)/chunk_size\n",
    "\n",
    "# Loop through user IDs in chunks\n",
    "for i in range(0, len(user_ids), chunk_size):\n",
    "    user_ids_chunk = user_ids[i:i+chunk_size]\n",
    "    remaining_iterations2 = chunk_size\n",
    "\n",
    "    # Get all the rows for the user_ids_chunk for all features at once\n",
    "    rows = conn.execute(\"SELECT user_id, previous, current, feature FROM changes WHERE user_id IN ({}) AND feature IN ({})\".format(','.join('?' for _ in user_ids_chunk), ','.join('?' for _ in features)), tuple(user_ids_chunk + features)).fetchall()\n",
    "\n",
    "    # Group the rows by user_id and feature\n",
    "    grouped_rows = {}\n",
    "    for row in rows:\n",
    "        grouped_rows.setdefault(row['user_id'], {}).setdefault(row['feature'], []).append(row)\n",
    "\n",
    "    # Create an array to store processed data for the chunk of users\n",
    "    processed_data_chunk = np.zeros((chunk_size*len(features)*len(new_features), 3), dtype=np.object)\n",
    "    j = 0\n",
    "\n",
    "    # Loop through each user in the chunk\n",
    "    for user_id in user_ids_chunk:\n",
    "        # Loop through each feature for the user\n",
    "        for k, feature in enumerate(features):\n",
    "            # Get the rows for the user_id and feature\n",
    "            rows = grouped_rows.get(user_id, {}).get(feature, [])\n",
    "\n",
    "            # Process the feature changes and get the values for new features\n",
    "            processed_feature_changes = process_feature_changes(rows, feature)\n",
    "\n",
    "            # Insert the values into the processed data array\n",
    "            for l, stat_name in enumerate(new_features):\n",
    "                value = processed_feature_changes.get(user_id, {}).get(stat_name, -1 if stat_name != \"num_changes\" and stat_name != \"skew_edit_distance\" else 0 if stat_name == \"num_changes\" else 100 if stat_name == \"skew_edit_distance\" else None)\n",
    "                processed_data_chunk[j, 0] = user_id\n",
    "                processed_data_chunk[j, 1] = \"Basic \" + feature + \" \" + stat_name\n",
    "                processed_data_chunk[j, 2] = value\n",
    "                j += 1\n",
    "                if j == len(new_features) :\n",
    "                    break\n",
    "\n",
    "        remaining_iterations2-=1\n",
    "        print((\"Iterations left:\", remaining_iterations2))\n",
    "        if j == chunk_size*len(features)*len(new_features):\n",
    "            break\n",
    "\n",
    "    # Insert the chunk of data into the database\n",
    "    cursor = conn.cursor()\n",
    "    # cursor.executemany(\"INSERT OR REPLACE INTO processed_data (user_id, feature_name, value, test_time) VALUES (?, ?, ?, ?)\", [(row[0], row[1], row[2], current_time) for row in processed_data_chunk if any(row)])\n",
    "    values = [(row[0], row[1], row[2], current_time) for row in processed_data_chunk if any(row)]\n",
    "    cursor.executemany(\n",
    "        \"INSERT OR REPLACE INTO processed_data (user_id, feature_name, value, test_time) VALUES (?, ?, ?, ?)\", values)\n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "    chunk_amount -= 1\n",
    "    print(\"chunk_left:\", chunk_amount)\n",
    "\n",
    "# Close the connection to the database\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
